{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = center>Neural Networks</h1>\n",
    "\n",
    "**Artificial neural networks (ANN)** or connectionist systems are computing systems that are inspired by, but not identical to, biological neural networks that constitute animal brains. \n",
    "\n",
    "Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with task-specific rules. \n",
    "\n",
    "**For example**, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process.\n",
    "\n",
    "\n",
    "An ANN is based on a collection of connected `units or nodes` called <b>artificial neurons</b>, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it.\n",
    "\n",
    "\n",
    "##### Biological motivation and connections\n",
    "\n",
    "The basic computational unit of the brain is a neuron. Approximately 86 billion neurons can be found in the human nervous system and they are connected with approximately $10^14 - 10^15$ synapses. The diagram below shows a cartoon drawing of a biological neuron (left) and a common mathematical model (right). Each neuron receives input signals from its dendrites and produces output signals along its (single) axon. The axon eventually branches out and connects via synapses to dendrites of other neurons. In the computational model of a neuron, the signals that travel along the axons (e.g. $x_0$) interact multiplicatively (e.g. $w_0x_0$) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. $w_0$). The idea is that the synaptic strengths (the weights  w) are learnable and control the strength of influence (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another. In the basic model, the dendrites carry the signal to the cell body where they all get summed. If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon. In the computational model, we assume that the precise timings of the spikes do not matter, and that only the frequency of the firing communicates information. Based on this rate code interpretation, we model the firing rate of the neuron with an activation function f, which represents the frequency of the spikes along the axon. Historically, a common choice of activation function is the sigmoid function $σ$, since it takes a real-valued input (the signal strength after the sum) and squashes it to range between 0 and 1. We will see details of these activation functions later in this section.\n",
    "\n",
    "<img src ='IMAGES/neuron.png'/>\n",
    "\n",
    "<hr>\n",
    "<img src = 'IMAGES/neuron_model.jpg'/>\n",
    "In ANN implementations, the \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called **edges**. Neurons and edges typically have a `weight that adjusts as learning proceeds`. The weight `increases` or `decreases` the strength of the signal at a connection. Neurons may have a `threshold` such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the **first layer (the input layer)**, to the **last layer (the output layer)**, possibly after traversing the layers multiple times.\n",
    "\n",
    "<img src = 'https://lh3.googleusercontent.com/-39A0GIv31hQ/Xa2l21EPmGI/AAAAAAAAF4g/paBlixHqMy4YQsMMDy0VfbltkVg694AVACK8BGAsYHg/s0/2019-10-21.jpg'>\n",
    "\n",
    "The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. \n",
    "\n",
    "\n",
    "ANNs have been used on a variety of tasks: \n",
    "1. Including computer vision, \n",
    "2. Speech recognition, \n",
    "3. Machine translation, \n",
    "5. Social network filtering, \n",
    "6. Playing board and video games, \n",
    "7. Medical diagnosis and even in activities that have traditionally been considered as reserved to humans, like painting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to introduce neural networks without appealing to brain analogies. In the section on linear classification we computed scores for different visual categories given the image using the formula  $s = W_x$ , where W was a matrix and  x was an input column vector containing all pixel data of the image. In the case of CIFAR-10, x  is a [3072x1] column vector, and W is a [10x3072] matrix, so that the output scores is a vector of 10 class scores.\n",
    "\n",
    "\n",
    "An example neural network would instead compute$s=W_2 max(0,W_1 x)$. Here, W1 could be, for example, a [100x3072] matrix transforming the image into a 100-dimensional intermediate vector. The function max(0,−)is a non-linearity that is applied elementwise. There are several choices we could make for the non-linearity (which we’ll study below), but this one is a common choice and simply thresholds all activations that are below zero to zero. Finally, the matrix $W_2$ would then be of size [10x100], so that we again get 10 numbers out that we interpret as the class scores. Notice that the non-linearity is critical computationally - if we left it out, the two matrices could be collapsed to a single matrix, and therefore the predicted class scores would again be a linear function of the input. The non-linearity is where we get the wiggle. The parameters $W_2,W_1$ are learned with stochastic gradient descent, and their gradients are derived with chain rule (and computed with backpropagation).\n",
    "\n",
    "A three-layer neural network could analogously look like  $s=W_3max(0,W2max(0,W1x))$, where all of $W_3,W_2,W_1$ are parameters to be learned. The sizes of the intermediate hidden vectors are hyperparameters of the network and we’ll see how we can set them later. Lets now look into how we can interpret these computations from the neuron/network perspective.\n",
    "\n",
    "#### Modeling one neuron\n",
    "\n",
    "The area of Neural Networks has originally been primarily inspired by the goal of modeling biological neural systems, but has since diverged and become a matter of engineering and achieving good results in Machine Learning tasks. Nonetheless, we begin our discussion with a very brief and high-level description of the biological system that a large portion of this area has been inspired by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs):\n",
    "    \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n",
    "    cell_body_sum = np.sum(inputs * self.weights) + self.bias\n",
    "    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function\n",
    "    return firing_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, each neuron performs a dot product with the input and its weights, adds the bias and applies the non-linearity (or activation function), in this case the sigmoid \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$σ(x)=1/(1+e−^x)$\n",
    ". We will go into more details about different activation functions at the end of this section.\n",
    "\n",
    "\n",
    "\n",
    "Coarse model. It’s important to stress that this model of a biological neuron is very coarse: For example, there are many different types of neurons, each with different properties. The dendrites in biological neurons perform complex nonlinear computations. The synapses are not just a single weight, they’re a complex non-linear dynamical system. The exact timing of the output spikes in many systems is known to be important, suggesting that the rate code approximation may not hold. Due to all these and many other simplifications, be prepared to hear groaning sounds from anyone with some neuroscience background if you draw analogies between Neural Networks and real brains. See this [Refference]('https://neurophysics.ucsd.edu/courses/physics_171/annurev.neuro.28.061604.135703.pdf'), or more recently this review if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
